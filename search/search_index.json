{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep Fold DB docs","text":"<p>DeepFold DB main rep delibae/DeepFold_DB.</p>"},{"location":"#libary","title":"Libary","text":"<ul> <li>gcputils: Big Query\uc640 Cloud Storage utils</li> </ul>"},{"location":"gcputils/gcputils/","title":"page","text":"<ul> <li>gcputils.bigquery  <ul> <li>gcputils.bigquery.BigQueryManager</li> </ul> </li> <li>gcputils.test<ul> <li>gcputils.test.connection_test</li> </ul> </li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/","title":"gcputils.bigquery.BigQueryManager","text":""},{"location":"gcputils/bigquery/BigQueryManager/#overview","title":"Overview","text":"<p>The <code>BigQueryManager</code> class provides an interface to interact with Google's BigQuery service. It requires the Google Cloud project ID during initialization and accepts a dry_run flag that if set to True, will only estimate the cost of query execution without actually executing them.</p> <p>You can use this class to execute SQL queries, write the results to a table in a dataset, extract a table to a Google Cloud Storage bucket, and estimate the cost of running a SQL query.</p>"},{"location":"gcputils/bigquery/BigQueryManager/#attributes","title":"Attributes","text":"<ul> <li><code>project_id (str)</code>: The Google Cloud project ID.</li> <li><code>dry_run (bool)</code>: If True, only the cost of queries will be estimated without actual execution.</li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/#methods","title":"Methods","text":""},{"location":"gcputils/bigquery/BigQueryManager/#__init__self-project_id-str-dry_run-bool-false","title":"<code>__init__(self, project_id: str, dry_run: bool = False)</code>","text":"<p>Initializes a new instance of the BigQueryManager class.</p> <p>Parameters:</p> <ul> <li><code>project_id (str)</code>: The Google Cloud project ID.</li> <li><code>dry_run (bool, optional)</code>: If True, it only estimates the cost of queries without actually executing them. Defaults to False.</li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/#howmuchself-query-str-job_config-optionalbigqueryqueryjobconfig-none","title":"<code>howmuch(self, query: str, job_config: Optional[bigquery.QueryJobConfig]) -&gt; None</code>","text":"<p>Estimates the cost and the amount of data that a query will process.</p> <p>Parameters:</p> <ul> <li><code>query (str)</code>: The SQL query to be estimated.</li> <li><code>job_config (Optional[bigquery.QueryJobConfig], optional)</code>: Job configuration for the BigQuery query job. Defaults to None.</li> </ul> <p>Returns:</p> <ul> <li>None</li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/#execute_queryself-query-str-optionaldataframe","title":"<code>execute_query(self, query: str) -&gt; Optional[DataFrame]</code>","text":"<p>Executes a SQL query and returns the results as a pandas DataFrame.</p> <p>Parameters:</p> <ul> <li><code>query (str)</code>: The SQL query to be executed.</li> </ul> <p>Returns:</p> <ul> <li><code>Optional[DataFrame]</code>: The result of the query as a pandas DataFrame or None if dry_run is True.</li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/#tobucketself-storage_dir-str-file_name-str-dataset_id-str-table_name-str-compression-bool","title":"<code>tobucket(self, storage_dir: str, file_name: str, dataset_id: str, table_name: str, compression: bool)</code>","text":"<p>Extracts a BigQuery table to a Google Cloud Storage bucket.</p> <p>Parameters:</p> <ul> <li><code>storage_dir (str)</code>: The Google Cloud Storage bucket where the table data will be extracted.</li> <li><code>file_name (str)</code>: The name of the file to store the extracted data in Google Cloud Storage.</li> <li><code>dataset_id (str)</code>: The ID of the dataset containing the source table.</li> <li><code>table_name (str)</code>: The name of the BigQuery table from which data will be extracted.</li> <li><code>compression (bool)</code>: If True, the extracted data will be compressed using GZIP; otherwise, no compression will be applied.</li> </ul> <p>Returns:</p> <ul> <li>None</li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/#writetableself-query-str-destination_dataset-str-destination_table_name-str","title":"<code>writetable(self, query: str, destination_dataset: str, destination_table_name: str)</code>","text":"<p>Executes a BigQuery SQL query and writes the results to a destination table in a dataset.</p> <p>Parameters:</p> <ul> <li><code>query (str)</code>: The SQL query to be executed.</li> <li><code>destination_dataset (str)</code>: The name of the destination dataset where the table will be created to store the query results.</li> <li><code>destination_table_name (str)</code>: The name of the destination table to store the query results.</li> </ul> <p>Returns:</p> <ul> <li>None</li> </ul>"},{"location":"gcputils/bigquery/BigQueryManager/#example","title":"Example","text":"<pre><code># Create a BigQueryManager instance\nmanager = gcputils.bigquery.BigQueryManager(project_id=\"your_project_id\", dry_run=True)\n\n# Use howmuch\nmanager.howmuch(query=\"your_query\", job_config=bigquery.QueryJobConfig())\n\n# Use execute_query\nresult_df = manager.execute_query(query=\"your_query\")\nprint(result_df)\n\n# Use tobucket\nmanager.tobucket(\n    storage_dir=\"storage_dir\",\n    file_name=\"file_name\",\n    dataset_id=\"dataset_id\",\n    table_name=\"table_name\",\n    compression=True,\n)\n\n# Use writetable\nmanager.writetable(\n    query=\"your_query\",\n    destination_dataset=\"destination_dataset\",\n    destination_table_name=\"destination_table_name\",\n)\n</code></pre>"},{"location":"gcputils/test/connection_test/","title":"gcputils.test.connection_test","text":""},{"location":"gcputils/test/connection_test/#overview","title":"Overview","text":"<p>The <code>connection_test</code> function tests the connection to Google Cloud Storage using Application Default Credentials. This function checks if the authentication to Google Cloud Storage works by using Application Default Credentials. These credentials are typically set up by running the 'gcloud auth application-default login' command.</p> <p>If the connection is successful, it will list the buckets associated with the authenticated project.</p>"},{"location":"gcputils/test/connection_test/#note","title":"Note","text":"<p>Before running this function, ensure you have authenticated your application with Google Cloud using 'gcloud auth application-default login' or by setting up the necessary environment variables.</p>"},{"location":"gcputils/test/connection_test/#raises","title":"Raises","text":"<ul> <li><code>google.auth.exceptions.DefaultCredentialsError</code>: If the function fails to authenticate using Application Default Credentials.</li> </ul>"},{"location":"gcputils/test/connection_test/#example","title":"Example","text":"<pre><code># Call the connection_test function to test the connection to Google Cloud Storage.\ngcputils.test.connection_test()\n</code></pre>"}]}